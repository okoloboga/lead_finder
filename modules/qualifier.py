import json
import logging
import re
import time
from typing import Dict, Any, List

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

import config

logging.basicConfig(
    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def _extract_json_payload(raw: str) -> str:
    """Extract JSON object from raw LLM output."""
    text = (raw or "").strip()

    if text.startswith("```"):
        lines = text.splitlines()
        if len(lines) >= 2:
            text = "\n".join(lines[1:])
        if text.endswith("```"):
            text = text[:-3]
        text = text.strip()

    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end != -1 and end > start:
        return text[start:end + 1]

    match = re.search(r"\{[\s\S]*\}", text)
    if match:
        return match.group(0)

    return text


def _parse_llm_json(raw: str) -> Dict[str, Any]:
    """Parse LLM response text as JSON."""
    return json.loads(_extract_json_payload(raw))


def _recover_partial_batch_response(
    raw: str, analyzed_count: int
) -> Dict[str, Any] | None:
    """Best-effort recovery for truncated batch JSON responses.

    If model output is cut in the middle, recover complete lead objects from
    the `potential_leads` array and return a valid minimal response.
    """
    text = (raw or "").strip()
    if not text:
        return None

    arr_key = '"potential_leads"'
    arr_idx = text.find(arr_key)
    if arr_idx == -1:
        return None

    arr_start = text.find("[", arr_idx)
    if arr_start == -1:
        return None

    # Parse complete JSON objects inside potential_leads array.
    leads: list[Dict[str, Any]] = []
    depth = 0
    obj_start = -1
    in_string = False
    escaped = False
    i = arr_start + 1
    while i < len(text):
        ch = text[i]

        if in_string:
            if escaped:
                escaped = False
            elif ch == "\\":
                escaped = True
            elif ch == '"':
                in_string = False
            i += 1
            continue

        if ch == '"':
            in_string = True
        elif ch == "{":
            if depth == 0:
                obj_start = i
            depth += 1
        elif ch == "}":
            if depth > 0:
                depth -= 1
                if depth == 0 and obj_start != -1:
                    chunk = text[obj_start : i + 1]
                    try:
                        obj = json.loads(chunk)
                    except json.JSONDecodeError:
                        obj = None
                    if isinstance(obj, dict) and obj.get("username"):
                        leads.append(obj)
                    obj_start = -1
        elif ch == "]" and depth == 0:
            break

        i += 1

    if not leads:
        return None

    selected = len(leads)
    return {
        "total_messages_analyzed": analyzed_count,
        "potential_leads": leads,
        "filtering_stats": {
            "analyzed": analyzed_count,
            "with_business_signals": selected,
            "with_pain_signals": selected,
            "selected_for_detailed_analysis": selected,
        },
        "recovered_from_truncated_json": True,
    }

# Initialize LLM once at the module level to save memory
try:
    llm = ChatOpenAI(
        openai_api_key=config.COMET_API_KEY,
        openai_api_base=config.COMET_API_BASE_URL,
        model=config.COMET_API_MODEL,
        temperature=0.5,
        request_timeout=60
    )
    logger.info("LLM client initialized successfully.")
except Exception as e:
    llm = None
    logger.error(f"Failed to initialize LLM client: {e}")


def load_qualification_prompt() -> str:
    """Loads the v2 qualification prompt from the file."""
    try:
        with open('prompts/qualification_v2.txt', 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        logger.error(
            "Qualification prompt file not found at 'prompts/qualification_v2.txt'"
        )
        return ""


def get_freshness_emoji(freshness: str) -> str:
    """Get emoji indicator for message freshness."""
    return {
        "hot": "üî•",
        "warm": "",
        "cold": "‚ö†Ô∏è",
        "stale": "ü•∂"
    }.get(freshness, "")


def format_messages_with_metadata(messages: List[Dict[str, Any]]) -> str:
    """Format messages with their links and freshness for the prompt."""
    if not messages:
        return ""

    lines = ["--- –°–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ —á–∞—Ç–µ (—Å –¥–∞—Ç–∞–º–∏ –∏ —Å—Å—ã–ª–∫–∞–º–∏) ---"]

    for msg in messages:
        freshness = msg.get("freshness", "unknown")
        emoji = get_freshness_emoji(freshness)
        age = msg.get("age_display", "–¥–∞—Ç–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞")
        text = msg.get("text", "")[:200]  # Limit text length
        link = msg.get("link", "")
        chat = msg.get("chat_username", "private chat")

        if emoji:
            line = f'- {emoji} [{age}] "{text}"'
        else:
            line = f'- [{age}] "{text}"'

        if link:
            line += f"\n  üìç {chat} | üîó {link}"

        lines.append(line)

    return "\n".join(lines) + "\n\n"


def format_enrichment_data_for_prompt(
    enrichment_data: Dict[str, Any],
    candidate_data: dict
) -> str:
    """Formats the enrichment data into a string for the LLM prompt."""
    prompt_text = ""

    # Use messages_with_metadata if available (new format)
    messages_with_metadata = candidate_data.get("messages_with_metadata", [])
    if messages_with_metadata:
        prompt_text += format_messages_with_metadata(messages_with_metadata)
    else:
        # Fallback to old format (sample_messages)
        sample_messages = candidate_data.get("sample_messages", [])
        if sample_messages:
            prompt_text += "--- –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ —á–∞—Ç–µ ---\n"
            for msg in sample_messages:
                prompt_text += f'- "{msg}"\n'
            prompt_text += "\n"

    if enrichment_data.get("channel_data"):
        ch_data = (enrichment_data["channel_data"] or {}).get("entity_data") or {}
        prompt_text += "--- –î–∞–Ω–Ω—ã–µ —Å –ª–∏—á–Ω–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞ ---\n"
        prompt_text += f"–ù–∞–∑–≤–∞–Ω–∏–µ: {ch_data.get('title', 'N/A')}\n"
        prompt_text += f"–ü–æ–¥–ø–∏—Å—á–∏–∫–æ–≤: {ch_data.get('participants_count', 'N/A')}\n"
        prompt_text += f"–û–ø–∏—Å–∞–Ω–∏–µ: {ch_data.get('about', 'N/A')}\n\n"

    if enrichment_data.get("web_search_data"):
        web_data = enrichment_data["web_search_data"]
        prompt_text += "--- –î–∞–Ω–Ω—ã–µ –∏–∑ –≤–µ–±-–ø–æ–∏—Å–∫–∞ ---\n"
        if web_data.get('website'):
            prompt_text += f"–ù–∞–π–¥–µ–Ω–Ω—ã–π —Å–∞–π—Ç: {web_data['website']}\n"
        mentions_str = "\n".join([
            f'- {m.get("title", "")} ({m.get("source", "")})'
            for m in web_data.get("mentions", [])
        ])
        if mentions_str:
            prompt_text += f"–£–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ —Å–µ—Ç–∏:\n{mentions_str}\n"
        prompt_text += "\n"

    return prompt_text


def get_freshness_summary(candidate_data: dict) -> Dict[str, Any]:
    """Get summary of message freshness for the lead card."""
    messages = candidate_data.get("messages_with_metadata", [])

    if not messages:
        return {
            "total_messages": 0,
            "has_hot": False,
            "has_warm": False,
            "freshest_age": None,
            "can_reply_in_chat": False
        }

    freshness_counts = {"hot": 0, "warm": 0, "cold": 0, "stale": 0}
    for msg in messages:
        freshness = msg.get("freshness", "stale")
        if freshness in freshness_counts:
            freshness_counts[freshness] += 1

    # Can reply in chat if any message has a link
    can_reply = any(msg.get("link") for msg in messages)

    return {
        "total_messages": len(messages),
        "has_hot": freshness_counts["hot"] > 0,
        "has_warm": freshness_counts["warm"] > 0,
        "freshest_age": messages[0].get("age_display") if messages else None,
        "can_reply_in_chat": can_reply,
        "freshness_breakdown": freshness_counts
    }


def qualify_lead(
    candidate_data: dict,
    enrichment_data: dict,
    niche: str,
    ai_ideas: str = "",
    user_services_description: str = "",
) -> dict:
    """
    Qualifies a lead using the pre-initialized LLM.

    Applies freshness bonuses to the score and returns comprehensive results.
    """
    if not llm:
        return {"error": "LLM client is not initialized."}

    prompt_template = load_qualification_prompt()
    if not prompt_template:
        return {"error": "Could not load qualification prompt."}
    services_text = (user_services_description or "").strip()
    if not services_text:
        services_text = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ —É–∫–∞–∑–∞–ª –æ–ø–∏—Å–∞–Ω–∏–µ —É—Å–ª—É–≥."
    prompt_template = prompt_template.replace("{services_description}", services_text)

    # Build input data
    input_data = (
        "--- –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ Telegram ---\n"
        f"–ù–∏—à–∞, –≤ –∫–æ—Ç–æ—Ä–æ–π –æ–Ω –Ω–∞–π–¥–µ–Ω: {niche}\n"
        f"–ò–º—è: {candidate_data.get('first_name', '')} "
        f"{candidate_data.get('last_name', '')}\n"
        f"Username: @{candidate_data.get('username', 'N/A')}\n"
        f"–ë–∏–æ: {candidate_data.get('bio', 'N/A')}\n"
        f"–°–æ–æ–±—â–µ–Ω–∏–π –≤ —á–∞—Ç–µ-–∏—Å—Ç–æ—á–Ω–∏–∫–µ: "
        f"{candidate_data.get('messages_in_chat', 'N/A')}\n\n"
    )
    input_data += format_enrichment_data_for_prompt(enrichment_data, candidate_data)
    if ai_ideas:
        input_data += ai_ideas

    system_message = SystemMessage(
        content=(
            "You are a business analyst expert in B2B lead qualification. "
            "Analyze the profile and provide a structured JSON output based "
            "on the provided schema. Do not add any text before or after "
            "the JSON object."
        )
    )
    human_message = HumanMessage(
        content=f"{prompt_template}\n\n–í–æ—Ç –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n{input_data}"
    )

    try:
        username = candidate_data.get('username', 'N/A')
        logger.info(f"Qualifying lead: @{username}. Waiting for LLM...")

        start_time = time.time()
        response = llm.invoke([system_message, human_message])
        end_time = time.time()
        duration = end_time - start_time

        logger.info(
            f"LLM response received for @{username}. "
            f"Call duration: {duration:.2f} seconds."
        )

        parsed_response = _parse_llm_json(response.content)
        logger.info(f"Successfully parsed LLM response for @{username}")

        # Get freshness summary for metadata (display only)
        freshness_summary = get_freshness_summary(candidate_data)

        # Get score from LLM
        qualification = parsed_response.get("qualification") or {}
        if isinstance(qualification, dict):
            llm_score = qualification.get("score", 0)
        else:
            llm_score = 0

        # Apply penalty: force score = 0 if LLM indicates we can't solve the problem
        reasoning = qualification.get("reasoning", "").lower()

        # Indicators that we CAN'T solve the problem with our bot
        cant_solve_indicators = [
            "–Ω–µ –º–æ–∂–µ–º —Ä–µ—à–∏—Ç—å", "–Ω–µ –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å", "–Ω–µ –º–æ–∂–µ–º –∑–∞–∫—Ä—ã—Ç—å",
            "–Ω–µ —Ä–µ—à–∞–µ—Ç—Å—è –±–æ—Ç–æ–º", "–Ω–µ —Ä–µ—à–∞–µ—Ç—Å—è –Ω–∞—à–∏–º", "–Ω–µ —Ä–µ—à–∞–µ—Ç—Å—è telegram",
            "–Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–æ—Ç–∞", "–±–æ—Ç –Ω–µ –º–æ–∂–µ—Ç", "–±–æ—Ç –Ω–µ —Ä–µ—à–∞–µ—Ç",
            "–Ω–µ—Ç api", "–Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω api", "api –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç", "–Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ api",
            "–Ω–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏", "–Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è",
            "–º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞", "–±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞",
            "–Ω–µ –ø—Ä–æ—Ü–µ—Å—Å", "–Ω–µ —Ä–µ—à–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑", "–Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö api",
            "–Ω—É–∂–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å", "—Ç—Ä–µ–±—É–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å",
            "–Ω–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –±–æ–ª–∏", "–Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –±–æ–ª–∏",
            "–±–æ–ª–∏ –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ", "–±–æ–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç"
        ]

        # Indicators that pain is vague/assumed (should also be score 0)
        vague_indicators = [
            "–Ω–µ—Ç –±–æ–ª–µ–π", "–Ω–µ—Ç –ø—Ä—è–º—ã—Ö –±–æ–ª–µ–π", "–Ω–µ –≤–∏–¥–Ω–æ –±–æ–ª–µ–π",
            "–±–æ–ª–∏ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–µ", "–±–æ–ª–∏ –Ω–µ –æ—á–µ–≤–∏–¥–Ω—ã",
            "—Ç–∏–ø–∏—á–Ω—ã–µ –±–æ–ª–∏", "–≤–µ—Ä–æ—è—Ç–Ω—ã–µ –±–æ–ª–∏", "–∫–æ—Å–≤–µ–Ω–Ω—ã–µ –±–æ–ª–∏",
            "–±–æ–ª–∏ —Ç–∏–ø–∏—á–Ω—ã–µ –¥–ª—è", "–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ", "–≤–µ—Ä–æ—è—Ç–Ω–æ –≤–ª–∞–¥–µ–ª–µ—Ü"
        ]

        has_cant_solve = any(indicator in reasoning for indicator in cant_solve_indicators)
        has_vague_reasoning = any(indicator in reasoning for indicator in vague_indicators)

        final_score = llm_score

        if has_cant_solve or has_vague_reasoning:
            # Force score to 0 if we can't solve or pain is vague
            final_score = 0
            logger.info(
                f"Forcing score to 0 for @{candidate_data.get('username')} "
                f"due to: {'cant_solve' if has_cant_solve else 'vague_reasoning'}. "
                f"Reasoning: {reasoning[:150]}"
            )

        # Update the qualification result with final score
        if isinstance(qualification, dict):
            qualification["llm_score"] = llm_score
            qualification["score"] = final_score
            qualification["penalty_applied"] = has_cant_solve or has_vague_reasoning
            qualification["penalty_reason"] = "cant_solve" if has_cant_solve else ("vague" if has_vague_reasoning else None)
            parsed_response["qualification"] = qualification

        # Add freshness metadata (for display, not scoring)
        parsed_response["freshness_summary"] = freshness_summary

        return {
            "llm_response": parsed_response,
            "raw_input_prompt": input_data,
            # Flatten for backward compatibility
            **parsed_response
        }

    except json.JSONDecodeError as e:
        username = candidate_data.get('username', 'N/A')
        logger.error(
            f"Failed to decode JSON from LLM response for @{username}: {e}"
        )
        logger.error(f"Raw response content: {response.content[:500]}")
        return {"error": "JSONDecodeError", "raw_response": response.content}
    except Exception as e:
        username = candidate_data.get('username', 'N/A')
        logger.error(
            f"An error occurred during lead qualification for @{username}: {e}"
        )
        return {"error": str(e)}


def load_batch_analysis_prompt() -> str:
    """Loads the batch chat analysis prompt from the file."""
    try:
        with open('prompts/chat_batch_analysis.txt', 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        logger.error(
            "Batch analysis prompt file not found at 'prompts/chat_batch_analysis.txt'"
        )
        return ""


def batch_analyze_chat(messages: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Analyzes an entire chat's messages in one LLM call to identify potential leads.

    This is the first stage of two-stage qualification:
    1. Batch screening (this function) - identifies candidates with pain/problems
    2. Detailed analysis (qualify_lead) - full qualification of selected candidates

    Args:
        messages: List of message dictionaries with keys:
            - username: str (e.g., "@username")
            - text: str (message text)
            - date: str (ISO format date)
            - messages_count: int (total messages from this user)

    Returns:
        Dictionary with:
        - potential_leads: List[Dict] with username, priority, pain_summary, etc.
        - filtering_stats: Dict with analysis statistics
        - error: str (if error occurred)
    """
    if not llm:
        return {"error": "LLM client is not initialized.", "potential_leads": []}

    prompt_template = load_batch_analysis_prompt()
    if not prompt_template:
        return {
            "error": "Could not load batch analysis prompt.",
            "potential_leads": []
        }

    # Format messages for the prompt
    messages_json = json.dumps(messages, ensure_ascii=False, indent=2)

    system_message = SystemMessage(
        content=(
            "You are a business analyst expert in B2B lead identification. "
            "Analyze the provided chat messages and identify potential leads "
            "for Telegram bot development services. "
            "Return ONLY valid JSON as specified in the prompt."
        )
    )
    human_message = HumanMessage(
        content=f"{prompt_template}\n\n–°–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n{messages_json}"
    )

    try:
        logger.info(
            f"Starting batch analysis of {len(messages)} messages. Waiting for LLM..."
        )

        start_time = time.time()
        response = llm.invoke([system_message, human_message])
        end_time = time.time()
        duration = end_time - start_time

        logger.info(
            f"Batch analysis LLM response received. "
            f"Call duration: {duration:.2f} seconds."
        )

        parsed_response = _parse_llm_json(response.content)
        logger.info(
            f"Successfully parsed batch analysis. "
            f"Found {len(parsed_response.get('potential_leads', []))} potential leads."
        )

        return parsed_response

    except json.JSONDecodeError as e:
        logger.error(f"Failed to decode JSON from batch analysis LLM response: {e}")
        logger.error(f"Raw response content: {response.content[:500]}")

        recovered = _recover_partial_batch_response(response.content, len(messages))
        if recovered:
            logger.warning(
                "Recovered partial batch analysis response from truncated JSON. "
                f"Recovered leads: {len(recovered.get('potential_leads', []))}."
            )
            return recovered

        try:
            retry_message = HumanMessage(
                content=(
                    f"{prompt_template}\n\n–°–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n{messages_json}\n\n"
                    "–í–ê–ñ–ù–û: –ø—Ä–æ—à–ª—ã–π –æ—Ç–≤–µ—Ç –±—ã–ª –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–º JSON. "
                    "–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ markdown/–ø–æ—è—Å–Ω–µ–Ω–∏–π. "
                    "–û–≥—Ä–∞–Ω–∏—á—å potential_leads –º–∞–∫—Å–∏–º—É–º 20, –æ—Ç–≤–µ—Ç —Å–¥–µ–ª–∞–π –∫–æ–º–ø–∞–∫—Ç–Ω—ã–º."
                )
            )
            retry_response = llm.invoke([system_message, retry_message])
            parsed_retry = _parse_llm_json(retry_response.content)
            logger.info(
                "Batch analysis retry succeeded. "
                f"Found {len(parsed_retry.get('potential_leads', []))} potential leads."
            )
            return parsed_retry
        except Exception as retry_e:
            logger.error(f"Batch analysis retry failed: {retry_e}")
            recovered_retry = _recover_partial_batch_response(
                retry_response.content if 'retry_response' in locals() else "",
                len(messages),
            )
            if recovered_retry:
                logger.warning(
                    "Recovered partial batch analysis from retry truncated JSON. "
                    f"Recovered leads: {len(recovered_retry.get('potential_leads', []))}."
                )
                return recovered_retry
            return {
                "error": "JSONDecodeError",
                "raw_response": response.content,
                "potential_leads": []
            }
    except Exception as e:
        logger.error(f"An error occurred during batch chat analysis: {e}")
        return {"error": str(e), "potential_leads": []}


if __name__ == '__main__':
    print("Qualifier module v2. To be tested as part of the main pipeline.")
